# -*- coding: utf-8 -*-
"""Week3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NRyg9eZP9Wl9hkeknzaroHfFC8RQM1Lg

# Week 3

I hope you're getting the hang of things. Today we're going on with the prinicples of data visualization!

## Overview

This week, we will work our way through four parts:

* First a lecture. You will watch a video on visualization and solve a couple of exercises.
* Second, some reading. We'll be reading about *scientific data visualization*, and the huge number of things you can do with just one variable. Naturally, we'll be answering questions about that book.
* Third, we'll be reproducing some of the plots from that book.
* And finally, we will get you going -- hands on -- with plotting geodata using Plotly. This is warming up for next time, when we'll dig deeper into geo-viz.

## I'm not here today, so a quick message to get things started

I'm taking this week off. You'll be well taken care of in the hands of the awesome [Laura Alessandretti](http://laura.alessandretti.com) who's a colleague from the Cogsys section and the fantastic TAs.

I do have some comments on the following practical topics around Sssignment 1. They're in the video, but I'll give you an outline of topics here so you know what's coming

 * When will the assignment posted (tl;dr: After Week 4's lecture)
 * Groups (tl;dr: full details here: https://github.com/suneman/socialdata2024/wiki/Groups)
 * How to hand in (tl;dr: via Learn)
 * Peer evaluation (tl;dr: not this time)
 * Communication will be via Teams ([link here](https://teams.microsoft.com/l/team/19%3ALxOjXoY8CXi2nYGlvYSJSOVAUjRQXUKo0KV1urpfSlg1%40thread.tacv2/conversations?groupId=4fd5f589-b507-48d4-b08d-ef4f4a37bafb&tenantId=f251f123-c9ce-448e-9277-34bb285911d9)).

[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/6fEcHd3s2kA/0.jpg)](https://www.youtube.com/watch?v=6fEcHd3s2kA)


AND ONE LAST THING. I'm trying to figure out how we're doing with the whole LLM thing. If you haven't already, please fill out the ultra-short, less-than-one-minute questionnaire I've set up. Your answers help, I use them to make the class better, so please, please, please. Pleasepleaseplease. Pleeeeeeeeeeaaaaaase. ü•∫ Please fill it out:

> https://forms.gle/KMG4g7SthjbBjkLH6

Thank you ‚ù§Ô∏è

## Part 1: Fundamentals of data visualization

Last week we had a small introduction of data visualization. Today, we are going to be a bit more specific on data analysis and visualization. Digging a bit more into the theory with the next video.

<mark>*You may feel tempted to skip the lectures on dataviz, but they are quite important. We don't have a formal book on data visualization. So the only source of knowledge about the **principles**, **theories**, and **ideas**, that are the foundation for good data viz, comes from the videos*. So **watch them** ü§ì </mark>

[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/yiU56codNlI/0.jpg)](https://www.youtube.com/watch?v=yiU56codNlI)

> *Excercise:* Questions for the lecture. <font color='grey'>Answer in your own words based on the video, ***don't use your LLM***.
> Once you've written down your answers, it's OK to use the LLM to get better.</font>
>
> * As mentioned earlier, visualization is not the only way to test for correlation. We can (for example) calculate the Pearson correlation. Explain in your own words how the Pearson correlation works and write down it's mathematical formulation. Can you think of an example where it fails (and visualization works)?
> * What is the difference between a bar-chart and a histogram?
> * I mention in the video that it's important to choose the right bin-size in histograms. But how do you do that? Do a Google search to find a criterion you like and explain it.
>
> <font color='grey'>It's easy to write a lot with LLMs, but here I want you to keep your answers short and precise. It's not important now, but if I ever ask you to hand in this exercise, I will add a word count. Probably something like 200 words total.</font>

## Part 2: Reading about the theory of visualization

Since we can go deeper with the visualization this year, we are going to read the first couple of chapters from [*Data Analysis with Open Source Tools*](http://shop.oreilly.com/product/9780596802363.do) (DAOST). It's pretty old, but I think it's a fantastic resource and one that is pretty much as relevant now as it was back then. The author is a physicist (like Sune) so he likes the way he thinks. And the books takes the reader all the way from visualization, through modeling to computational mining. Anywho - it's a great book and well worth reading in its entirety.

As part of this class we'll be reading the first chapters. Today, we'll read chaper 2 (the first 28 pages) which supports and deepens many of the points we made during the video above.

To find the text, you will need to go to **[Teams](https://teams.microsoft.com/l/team/19%3ALxOjXoY8CXi2nYGlvYSJSOVAUjRQXUKo0KV1urpfSlg1%40thread.tacv2/conversations?groupId=4fd5f589-b507-48d4-b08d-ef4f4a37bafb&tenantId=f251f123-c9ce-448e-9277-34bb285911d9)** and have a look under the `Files` tab. The text is in `Week2/DAOST_chapter1.pdf`.

> *Excercise*: Questions for DAOST <font color="gray">As always, try to answer without the help of your LLM and through reading the text, but feel free to add more nuance by asking it questions.</font>
> * Explain in your own words the point of the jitter plot.
> * Explain in your own words the point of figure 2-3. (I'm going to skip saying "in your own words" going forward, but I hope you get the point; I expect all answers to be in your own words).
> * The author of DAOST (Philipp Janert) likes KDEs (and think they're better than histograms). And we don't. Sune didn't give a detailed explanation in the video, but now that works to our advantage. We'll ask you to think about this and thereby create an excellent exercise: When can KDEs be misleading?
> * Sune discussed some strengths of the CDF - there are also weaknesses. Janert writes "CDFs have less intuitive appeal than histograms of KDEs". What does he mean by that?
> * What is a *Quantile plot*? What is it good for.
> * How is a *Probablity plot* defined? What is it useful for? Have you ever seen one before?
> * One of the reasons we like DAOST is that Janert is so suspicious of mean, median, and related summary statistics. Explain why one has to be careful when using those - and why visualization of the full data is always better.
> * Sune loves box plots (but not enough to own one of [these](https://images.app.goo.gl/rpozyRX3xu5oFobt8) üòÇ). When are box plots most useful?
> * The book doesn't mention [violin plots](https://en.wikipedia.org/wiki/Violin_plot). Are those better or worse than box plots? Why?
> * Remember the box-plot part from [this video from last time](https://www.youtube.com/watch?v=DbJyPELmhJc) (the part that starts at 0:56)? Explain in your own words how this video illustrates potential issues even with box-plots? Do violin-plots help with that issue?

## Part 3: Visualizations based on the book

> *Excercise Part 1*: Connecting the dots and recreating plots from DAOST but using our own favorite dataset.
>
> <font color="grey">For the two parts below, I want you to eliminate uses of LLMs where you simply cut and paste the assignments (or parts of it) into a prompt ... and ask for a solution. Instead, ask it for help if you get stuck ... use it as a TA to help understand error messages ... or to help make suggestions for the right function to use ... etc. </font>
>
> * Let's make a jitter-plot (that is, code up something like **Figure 2-1** from DAOST from scratch), but based on *SF Police data*. My hunch from inspecting the file is that the police-folks might be a little bit lazy in noting down the **exact** time down to the second. So choose a crime-type and a suitable time interval (somewhere between a month and 6 months depending on the crime-type) and create a jitter plot of the arrest times during a single hour (like 13-14, for example). So let time run on the $x$-axis and create vertical jitter.
> * Last time, we did lots of bar-plots. Today, we'll play around with histograms (creating two crime-data based versions of the plot-type shown in DAOST **Figure 2-2**). I think the GPS data could be fun to see this way.
>   * This time, pick two crime-types with different geographical patterns **and** a suitable time-interval for each (you want between 1000 and 10000 points in your histogram)
>   * Then take the latitude part of the GPS coordinates for each crime and bin the latitudes so that you have around 50 bins across the city of SF. You can use your favorite method for binning. I like `numpy.histogram`. This function gives you the counts and then you do your own plotting.
> * Next up is using the plot-type shown in **Figure 2-4** from DAOST, but with the data you used to create Figure 2.1. There is not a single great way to create kernel density plots in Python. [Here](https://www.geeksforgeeks.org/density-plots-with-pandas-in-python/), you'll finde a pandas based strategy, but you can also use `gaussian_kde` from `scipy.stats` ([for an example, check out this stackoverflow post](https://stackoverflow.com/questions/4150171/how-to-create-a-density-plot-in-matplotlib)) or you can use [`seaborn.kdeplot`](https://seaborn.pydata.org/generated/seaborn.kdeplot.html). There is also another nice tutorial for KDE plots [here](https://towardsdatascience.com/histograms-and-density-plots-in-python-f6bda88f5ac0). <font color="gray">Or ask your LLM what it recommends</font>.
> * Now grab 25 random timepoints from the dataset (of 1000-10000 original data) you've just plotted and create a version of Figure 2-4 based on the 25 data points. Does this shed light on why I think KDEs can be misleading?
>
> Let's take a break. Get some coffee or water. Stretch your legs. Talk to your friends for a bit. Breathe. Get relaxed so you're ready for the second part of the exercise.
>
>
> *Excercise Part 2*:
>
> * Now we'll work on creating two versions of the plot in **Figure 2-11**, but using the GPS data you used for your version of Figure 2-2. Comment on the result. It is not easy to create this plot from scracth.    
>   * **Hint:** Take a look at the `scipy.stats.probplot` function.
> * OK, we're almost done, but we need some box plots. Here, I'd like you to use the box plots to visualize fluctuations of how many crimes happen per day. We'll use data from the 15 focus crimes defined last week.
>   * For the full time-span of the data, calulate the **number of crimes per day** within each category for the entire duration of the data.
>   * Create a box-and whiskers plot showing the mean, median, quantiles, etc for all 15 crime-types side-by-side. There are many ways to do this. I like to use [matplotlibs's built in functionality](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.boxplot.html), but you can also achieve good results with [seaborn](https://seaborn.pydata.org/generated/seaborn.boxplot.html) or [pandas](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.boxplot.html). <font color="gray">It's also fine to get help from your LLM, but make sure you check that the plot accurately shows the stats about the underlying data!</font>.
>   * What does this plot reveal that you can't see in the plots from last time?
> * Also I want to show you guys another interesting use of box plots. To get started, let's calculate another average for each focus-crime, namely what time of day the crime happens. So this time, the distribution we want to plot is the average time-of-day that a crime takes place. There are many ways to do this, but let me describe one way to do it.
>   * For datapoint, the only thing you care about is the time-of-day, so discard everything else.
>   * You also have to deal with the fact that time is annoyingly not divided into nice units that go to 100 like many other numbers. I can think of two ways to deal with this.
>     * For each time-of-day, simply encode it as seconds since midnight.
>     * Or keep each whole hour, and convert the minute/second count to a percentage of an hour. So 10:15 $\rightarrow$ 10.25, 8:40 $\rightarrow$ 8.67, etc.
> * Now you can create box-plots to create an overview of *when various crimes occur*. Note that these plot have quite a different interpretation than ones we created in the previous exercise. Cool, right?
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

data = pd.read_csv("/content/Police_Department_Incident_Reports__Historical_2003_to_May_2018_20240221.csv")
data.describe()
data.head()
data_num = data.shape[0]

# ËæìÂá∫ÁªìÊûú
print(data_num)

from matplotlib import pyplot as plt
_df_16['PdId'].plot(kind='line', figsize=(8, 4), title='PdId')
plt.gca().spines[['top', 'right']].set_visible(False)

from matplotlib import pyplot as plt
import seaborn as sns
def _plot_series(series, series_name, series_index=0):
  from matplotlib import pyplot as plt
  import seaborn as sns
  palette = list(sns.palettes.mpl_palette('Dark2'))
  xs = series['Date']
  ys = series['PdId']

  plt.plot(xs, ys, label=series_name, color=palette[series_index % len(palette)])

fig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')
df_sorted = _df_12.sort_values('Date', ascending=True)
for i, (series_name, series) in enumerate(df_sorted.groupby('Category')):
  _plot_series(series, series_name, i)
  fig.legend(title='Category', bbox_to_anchor=(1, 1), loc='upper left')
sns.despine(fig=fig, ax=ax)
plt.xlabel('Date')
_ = plt.ylabel('PdId')

from matplotlib import pyplot as plt
import seaborn as sns
_df_4.groupby('Category').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

from matplotlib import pyplot as plt
_df_0['PdId'].plot(kind='hist', bins=20, title='PdId')
plt.gca().spines[['top', 'right',]].set_visible(False)

crime_type = "ROBBERY"
filtered_data = data[data["Category"] == crime_type]

interval = pd.Timedelta(days=30)

filtered_data["Time"] = pd.to_datetime(filtered_data["Time"])
# Filter data for a specific hour
start_time = pd.to_datetime("13:00")
end_time = start_time + pd.Timedelta(hours=1)
filtered_data = filtered_data[(filtered_data["Time"] >= start_time) & (filtered_data["Time"] < end_time)]

filtered_data["arrest_time_jitter"] = filtered_data["Time"] + pd.to_timedelta(np.random.randn(len(filtered_data)) * 5)  # Adjust noise as needed

plt.figure(figsize=(8, 6))
plt.scatter(filtered_data["arrest_time_jitter"].dt.hour, filtered_data.index)
plt.xlabel("Hour of Arrest (13-14)")
plt.ylabel("Arrest Index")
plt.title(f"Jitter Plot of {crime_type} Arrests in SFPD (13-14)")
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats

# Load your crime data (replace with your actual file path)
data = pd.read_csv("/content/Police_Department_Incident_Reports__Historical_2003_to_May_2018_20240221.csv")

# Choose a crime type (if needed)
crime_type = "ROBBERY"  # Replace with your desired crime type
filtered_data = data[data["Category"] == crime_type]

# Extract latitude data
latitudes = filtered_data["Y"]

# **Correct usage of probplot**
plt.figure()  # Create a new figure before plotting

# Pass the plot function **without parentheses or quotes**
probplot(latitudes, dist="norm", plot=plt)

plt.xlabel("Theoretical Quantiles (Normal Distribution)")
plt.ylabel("Ordered Latitude Values")
plt.title(f"Q-Q Plot of Latitude for {crime_type} (Normal Distribution)")
plt.grid(True)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Load your crime data (replace with your actual file path)
data = pd.read_csv("/content/Police_Department_Incident_Reports__Historical_2003_to_May_2018_20240221.csv")

# Choose the numerical variable to analyze (replace if needed)
variable_name = "X"
data["X"] = [float(x) for x in data["X"]]

# Group data by crime type
grouped_data = data.groupby("Category")[variable_name]

# Create the box-and-whiskers plot
plt.figure(figsize=(12, 6))  # Adjust figure size as needed

# Use pandas methods to calculate boxplot data
boxes = grouped_data.quantile([0.25, 0.5, 0.75])
whiskers = grouped_data.apply(pd.qcut, q=[0, 0.05, 0.95, 1], labels=["outlier_lower", "lower", "upper", "outlier_upper"])
medians = grouped_data.median()

# Create the plot elements
boxes.plot(kind="box", vert=False, ax=plt.gca())  # Draw boxes (transpose for better layout)
whiskers.plot(kind="scatter", x="Category", y="variable", style="o", ax=plt.gca(), color="red")  # Plot outliers
medians.plot(kind="line", style="-", x="Category", y="variable", color="blue", ax=plt.gca())  # Plot medians

# Customize the plot (optional)
plt.xlabel("Crime Type")
plt.ylabel(variable_name)
plt.title(f"Distribution of {variable_name} Across Crime Types")
plt.xticks(rotation=45, ha="right")  # Rotate x-axis labels for readability
plt.grid(True)
plt.tight_layout()

plt.show()

from urllib.request import urlopen
import json
with urlopen('https://raw.githubusercontent.com/suneman/socialdata2022/main/files/sfpd.geojson') as response:
    counties = json.load(response)

counties["features"][0]

"""## Part 4: Visualizing geodata with Plotly

So visualizing geodata used to be difficult, but with `Plotly` things have gotten easier.

Like matplotlib, Plotly is an [open-source data visualization library](https://plotly.com/python/), but it's aimed at making interactive visualizations that can be rendered in a web browser (or jupyter notebook). You can read about it and learn how to install it [here](https://plotly.com/python/getting-started/).

That means that we can easily draw on the fact that the crime data has lots of exciting geo-data attached. The map we're going to be creating is called a **[choropleth map](https://en.wikipedia.org/wiki/Choropleth_map)** (more on these later), which is basically a map, where we color in shapefiles (more on this below) based on some value that we care about. We'll take our inspiration from Plotly's gentle intro to [Choropleth maps](https://plotly.com/python/mapbox-county-choropleth/)

The thing we want to look into is the SF police districts, shown below (image stolen from [this page](https://hoodline.com/2015/07/citywide-sfpd-redistricting-to-take-effect-sunday/)).

![districts from web](https://raw.githubusercontent.com/suneman/socialdata2021/master/files/sfpdfinal.png)

But because we are cool programmers, we want to create our own maps, **with our own information on them**. Let's do it!

> *Exercise*: Let's plot a map with some random values in it. <font color="gray">For this one, it might be tempting to ask your LLM for a lot of help ... but don't let it do the work for you. Use it as a helper - not an end-to-end solution. There are many new concepts to understand, and doing the exercise yourself is the best way to begin to get the nitty-gritty details of maps and shapfiles under your skin.</font>
>
> What we need to do to get going is some random data. Below is a little dictionary with a random value for each district.
"""

randomdata = {
    'CENTRAL': 0.283805288999638,
    'SOUTHERN': 0.8882636532075772,
    'BAYVIEW': 0.45059924801053985,
    'MISSION': 0.6000904430914474,
    'PARK': 0.6362552416309091,
    'RICHMOND': 0.3371857964893169,
    'INGLESIDE': 0.09876749056377487,
    'TARAVAL': 0.009436215026031758,
    'NORTHERN': 0.44884916837512767,
    'TENDERLOIN': 0.06616710190569974
}

import pandas as pd
import plotly.express as px

# Create DataFrame from dictionary
data = pd.DataFrame.from_dict(randomdata, orient="index", columns=["value"])

# Normalize data to percentages
data["value"] = data["value"] / data["value"].sum() * 100

# Create the Choropleth map
fig = px.choropleth_mapbox(
    data_frame=data,
    locations=data.index,
    color="value",
    mapbox_style="open-street-map",
    zoom=7,  # Adjust zoom level as needed
    center={"lat": 37.7749, "lon": -122.4194},  # Center map on San Francisco
)

# Add title and show the plot
fig.update_layout(title="Choropleth Map of Random Data")
fig.show()

from urllib.request import urlopen
import json
with urlopen('https://raw.githubusercontent.com/suneman/socialdata2022/main/files/sfpd.geojson') as response:
    counties = json.load(response)

counties["features"][0]

import pandas as pd
data = pd.read_csv("/content/Police_Department_Incident_Reports__Historical_2003_to_May_2018_20240221.csv")

data_num=data.shape[0]
# ËæìÂá∫ÁªìÊûú
print(data_num)

import plotly.express as px

fig = px.choropleth_mapbox(data, geojson=counties, locations='PdDistrict', color='Category',
                           color_continuous_scale="Viridis",
                           range_color=(0, 1),
                           mapbox_style="carto-positron",
                           zoom=3, center = {"lat": 37.0902, "lon": -95.7129},
                           opacity=0.5,
                           labels={'Category':'unemployment rate'}
                          )
fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
fig.show()

"""> *Exercise* (continued):
>
> For this exercise, we'll use use the random values above and we'll also need some *shape-files*.
> [Shapefiles can have many different formats](https://en.wikipedia.org/wiki/Shapefile). Because we are brilliant teachers and an all-round standup people, we are sharing the shapefiles as [`geojson`](https://en.wikipedia.org/wiki/GeoJSON), which is an easy-to-use format for shapefiles based on `json`.
>
> * Download the SFPD District shapefiles **[here](https://raw.githubusercontent.com/suneman/socialdata2022/main/files/sfpd.geojson)**
> * Now that you have the shapefiles, you can follow the example here: https://plotly.com/python/mapbox-county-choropleth/ but with the following modifications:
>    * In the example the `id` is a so-called FIPS code. In our case the `id` is the `DISTRICT`
>    * You will have to convert the dictionary of random values I included above to a Pandas dataframe with the right column headings.
>    * The data used in the example has a range between zero and 12. Your data is between $[0,1]$. So you'll need to modify the plotting command to accound for that change.
>    * You should also change the map to display the right zoom level.
>    * And the map should center on San Francisco's `lat` and `lon`.
> * Now you can create your map.

Mine looks something like this (but I've changed the random values to make it more fun, so expect your colors to be different.).

![map_example.png](https://raw.githubusercontent.com/suneman/socialdata2021/master/files/map_example.png)

You're encouraged to play around with other settings, color schemes, etc.

> *Exercise:* But it's crime-data. Let's do something useful and **visualize where it is safest to leave your car on a Sunday**.
>
> Take a moment to congratulate yourself. You now know how to create cool map plots!
> * Now, we can focus on our main goal: *determine the districts where you should (and should not) leave your car on Sundays*. (Or stated differently, count up the number of thefts.)
> * To do so, first:
>  * Filter the crime dataset by the `DayOfWeek` category and also choose the appropriate crime category.
>  * Aggregate data by police district.
> * To create the plot, remember that your range of data-values is different from before, so you'll have to change the plotly command a bit.
> * **Based on your map and analysis, where should you park the car for it to be safest on a Sunday? And where's the worst place?**
> * Using visualizatios can help us uncover powerful data-patterns. However, when designing visualizations, we need to be aware of several illusions that can lead viewers to misinterpret the data we are showing (i.e. *perceptual errors*):
>    * Try to change the range of data-values in the plot above. Is there a way to make the difference between district less evident?
>    * Why do you think perceptual errors are a problem? Try to think of a few examples. You can have a look at this [article](https://www.businessinsider.com/fox-news-obamacare-chart-2014-3?r=US&IR=T) to get some inspiration.
> * <font color="gray">Optional LLM challenge: Use your LLM and your existing code to easily create plots for the same crime type, but different days, and comment on the results.</font>
"""

import pandas as pd
import plotly.express as px
filtered_df = data[(data["DayOfWeek"] == "Sunday") & (data["Category"] == "VEHICLE THEFT")]
filtered_df.head()
data_num = filtered_df.shape[0]
df_num=data.shape[0]
# ËæìÂá∫ÁªìÊûú
print(df_num)
print(data_num)

theft_counts_by_district = filtered_df.groupby("PdDistrict").size().to_frame(name="Thefts")
theft_counts_by_district["Theft Rate"] = theft_counts_by_district["Thefts"] / theft_counts_by_district["Thefts"].sum() * 100
theft_counts_by_district = theft_counts_by_district.reset_index()
theft_counts_by_district.head()

import pandas as pd
import plotly.express as px

with urlopen('https://raw.githubusercontent.com/suneman/socialdata2022/main/files/sfpd.geojson') as response:
    geojson_data = json.load(response)

fig = px.choropleth_mapbox(
    theft_counts_by_district,
    geojson=geojson_data,
    locations="PdDistrict",
    color="Thefts",
    mapbox_style="carto-positron",
    zoom=10,
    center={"lat": 37.7749, "lon": -122.4194},
    title="The number of vehivle in different district on sunday",
    labels={"Thefts": "Theft number"}
)

fig.show()